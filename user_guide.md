# User Guide: Real-Time Data Ingestion with Spark Structured Streaming & PostgreSQL

This guide explains how to set up and run the complete real-time data pipeline using Docker, Spark, and PostgreSQL.

---

## Prerequisites

- Docker & Docker Compose installed
- Python 3.x installed
- Internet connection to download required Docker images and Python packages

---

## Project Structure
```

spark-streaming-postgres/ 
├── data/ 
│     ├── input/ # CSV files generated by data_generator.py 
│     └── checkpoints/ # Spark checkpointing 
├── jars/ 
│     └── postgresql-42.7.3.jar # JDBC driver 
├── docker-compose.yml 
├── data_generator.py 
├── spark_streaming_to_postgres.py 
├── postgres_setup.sql 
├── postgres_connection_details.txt 
├── docs/ 
│    ├── project_overview.md 
│    └──
```
---
## Step-by-Step Instructions

### 1. Clone the Project and Navigate to the Directory
```bash
cd spark-streaming-postgres
```
### 2. Create Required Folders
```bash
mkdir -p data/input data/checkpoints jars
```
### 3. Download the PostgreSQL JDBC Driver
```bash
Invoke-WebRequest https://jdbc.postgresql.org/download/postgresql-42.7.3.jar -OutFile .\jars\postgresql-42.7.3.jar
```
- Or manually download it and place it in the jars/ folder

### 4. Start the Docker Containers
```bash
docker-compose up -d
```
This launches:

- postgres_db on port 5432

- spark_master with Spark Structured Streaming

### 5. Copy Files into the Spark Container
```bash
docker cp .\spark_streaming_to_postgres.py spark_master:/opt/spark_streaming_to_postgres.py
docker cp .\jars\postgresql-42.7.3.jar spark_master:/opt/postgresql-42.7.3.jar
```
#### **NB: In two separate terminals perform 6 and 7**
---
### 6. Run the Data Generator
```bash
python data_generator.py
```
### 7. Run the Spark Streaming Job
```bash
docker exec -it spark_master spark-submit \
  --jars /opt/postgresql-42.7.3.jar \
  /opt/spark_streaming_to_postgres.py
```
Spark will continuously watch `/opt/data/input/` (mapped from ./data/input/) and write cleaned data into PostgreSQL.

### 8. Verify the Data in PostgreSQL
```bash
docker exec -it postgres_db psql -U sparkuser -d ssparkdb
```
Run:
```sql
SELECT * FROM user_events LIMIT 10;
```

### 9. Stop the Pipeline
* Press Ctrl+C to stop the Spark job or data generator

* Stop containers:
```bash
docker-compose down
```
---

### Optional: Recreate the DB Table
If needed:
```
docker exec -it postgres_db psql -U sparkuser -d ssparkdb -f /path/to/postgres_setup.sql
```
---

#### Notes
* Spark Web UI: http://localhost:8080

* PostgreSQL connects via `host=localhost` outside Docker, or `host=postgres_db` inside the container



